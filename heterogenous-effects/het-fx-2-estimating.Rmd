---
title: "Characterizing Effect Heterogeneity 2"
author: "Cyrus Samii"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
    includes:
      in_header: preamble.tex
indent: true
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Paths to wd for users
knitr::opts_knit$set(root.dir="~/documents/github/cds-demos/heterogenous-effects")
library(rio)
library(knitr)
library(Emcdf)
library(causalTree)
source("~/documents/github/cds-demos/analysis-functions/analysis-functions.R")
```

\clearpage

# Estimating Conditional Treatment Effects (CATES) 

## Setting

Some notation following Chernozhukov et al.:

- Potential outcomes, $Y(1), Y(0)$.
- Covariates $X$.
- Baseline conditional average (BCA): $b_0(X) = E[Y(0)|X]$.
- CATE: $s_0(X) = E[Y(1)|X] - E[Y(0)|X]$.
- CIA holds: $D \independent (Y(1), Y(0)) | X$.
- Subvector of stratifying covariates, $X_1 \subseteq X$, such that
- propensity score is $p(X)=P[D=1|X_1]$, with $0 < p_0 \le p(X) \le p_1 < 1$.
- Observe $Y= DY(1) + (1-D)Y(0)$, in which case,
- $Y = b_0(X) + Ds_0(X) + U$, with $E[U|X,D] = 0$, where
- $b_0(X) = E[Y|D=0,X]$ and $s_0(X) = E[Y|D=1, X] - E[Y|D=0, X]$. CIA allows us to write $b_0$ and $s_0$ in terms of observables.
- Observe $N$ iid draws of $(Y,X,D)$ with law $P$. Draws are indexed by $i=1,...,N$.

A result to keep in mind (already noted in Athey and Imbens PNAS, I believe):

Define
$$
H = H(D,X) = \frac{D-p(X)}{p(X)(1-p(X))}.
$$
Given the assumptions above, we have, 
\begin{align*}
E[YH|X] & = E\left[\frac{Y(D-p(X))}{p(X)(1-p(X))} \bigg| X \right]\\
& = \frac{1}{p(X)(1-p(X))}E\left[ D(D-p(X))Y(1) + (1-D)(D-p(X))Y(0) | X \right]\\
& = E[Y(1)|X] - E[Y(0)|X] \\
& = s_0(X).
\end{align*}

We can use $YH$ as an "unbiased signal" about $s_0(X)$.  It is, however, a noisy signal, and so in using this, Chernozhukov et al. make adjustments (e.g., for their second BLP method).  Nonetheless, it does provide a target that one can use in trying to tune methods for estimating CATEs (see below the section on choosing an ML method).



## Overview and Goals

We consider three types of high-dimensional methods for estimating heterogenous treatment effects: trees, random forests, and ``elastic net'' regularized regression.  The focus initially will be on point predictions, rather than inference on such predictions. This is because the applications that we use work with summaries of the point predictions rather than the point predictions in and of themselves.  See the section below on "features of CATES", referencing Chernozhukov et al. (2017, arxiv), for more on this point. 

We are also interested in methods that work well when we entertain a high-dimensional covariate vector.  I say "entertain" because it may be that, in fact, the covariates that predict heterogeneity are few, but this is something that we do not know *a priori,* and rather we have at our disposal many covariates that we want to consider as candidates for predicting effect heterogeneity.  This leads us to machine learning approaches that use regularization to balance that ability to make very fine grained predictions with penalties for overfitting.

We consider the following algorithms:

- Trees
- Random Forests
- Elastic Net

## Data preparation

Bringing in Data (note that we need to harmonize data wrt to section 1):
```{r}
mex_data <- as.data.frame(import("progresa_mat.dta"))
covs <- setdiff(names(mex_data),
                c('indiv_id', 
                  'treatment', 
                  'enrolled', 
                  'treated_adj', 
                  'y_adj',
                  'ml_single_parent',
                  'group'))
```

Checking the data:
```{r}
misCheck <- as.matrix(apply(mex_data, 2, function(x){sum(is.na(x))}))
colnames(misCheck) <- "Number missing" 
kable(misCheck)
```

\clearpage
# Trees

## Trees for potential outcomes

We start with using a regression tree to model potential outcomes.  This helps us to understand how the trees work and the settings that we need to fix. We just use all available covariates.  The `rpart()` function loads when we load the `causalTree` package. We will use 10-fold cross validation (CV) to estimated CV-error that we will minimize in selecting the complexity parameter to prune the tree. 

We evaluate covariate importance using the built-in evaluation function.  For a classification problem such that this, a given covariate's importance is based on the sum of standardized goodness of classification values for all splits that involves it.

Here is the function:
```{r}
print(rpartOp)
```

And now results:

```{r}
mexFormula <- as.formula(paste("enrolled ~", paste(covs, collapse="+")))
mex_data1 <- subset(mex_data, treatment==1)
mex_data0 <- subset(mex_data, treatment==0)
tree_y1 <- rpartOp(formulaIn=mexFormula,
                    methodIn="class",
                    dataIn=mex_data1,
                    xvalIn=10,
                    minbucketIn=2,
                    cp_incIN=0,
                    target="po")
rpart.plot(tree_y1$tree)
par(mar=c(4,10,2,2))
rpart_impplot(treeIn=tree_y1, mainIn="Y1", labelScale=0.75)
```

```{r}
tree_y0 <- rpartOp(formulaIn=mexFormula,
                    methodIn="class",
                    dataIn=mex_data0,
                    xvalIn=10,
                    minbucketIn=2,
                    cp_incIN=0,
                    target="po")
rpart.plot(tree_y0$tree)
par(mar=c(4,10,2,2))
rpart_impplot(treeIn=tree_y0, mainIn="Y0", labelScale=0.75)
```

Looking at the results for the treated and control outcomes, we see that in both cases, the optimal trees gain predictive traction almost entirely by splitting on age and years of education. The other variables contribute to predictive accuracy only to a neglible degree.

## Interpreting the `rpart` fit

Here is a primer on what `rpart` is doing.  

First, we can see a textual representation of the tree.  Child nodes are always $(2x, 2x+1)$ of a parent $x$.
```{r}
print(tree_y1$tree)
```

The detailed output is given by `summary`.  

* At the top are the results of the CV assessment of error.  We use this to find the CV-optimal tree.   
* Then we have the scaled (to sum to 100) variable importance scores for variables with scaled scores of at least one.
* Then we have information on the candidate splits at each node going down the tree.  Here are details for the binary classification case for a node, which we will refer to as node $A$.
  - We have number of observations in node $A$, a complexity parameter for this split, an indicator for what class is being predicted, and then expected loss is just $1-p_1(A)$, where $p_1(A) = \Pr[Y=1|A]$, and then the probability that a random unit would arrive at this node.
  - Candidate splits are ordered on the basis of their contributions to increasing predictive accuracy. The resulting tree just uses the first of these. We have "primary" and "surrogate" splits listed. The "surrogates" are what one uses if the "primary" variable exhibits missing data.  If there is no missingness, one just uses the "primary".
  - For each of the candidate splits, we have an "improve" measure.  These are what we add up to determine variable importance.  As per the `rpart` vignette, the "improve" measure equals $N$ times the change in the impurity.  The default impurity function for binary classification uses Gini ($f(p)=p(1-p)$) and thus defines impurity at node $A$ as
$$
I(A) = p_0(A)(1-p_0(A)) + p_1(A)(1-p_1(A)) = 2p_1(A)(1-p_1(A)).
$$
Let $C_{k}(A)$ be the optimal split at node $A$ for variable $k$. Then, the change in impurity after splitting at node $A$ on the basis of $C_{k}(A)$ is given by
$$
\Delta_I( C_{k}(A), A) = I(A) - \Pr[A_L(C_{k}(A))]*I(A_L(C_{d,k})) - \Pr[A_R(C_{d,k})]*I(A_R(C_{k}(A))). 
$$
Finally, the "improve" score is given by $N(A)\Delta_I( C_{k}(A), A)$.  We can show the calculation for the first and second nodes:
```{r}
# improve at node 1:
N_A <- 10361
I_A <- 2*0.79432487*(1-0.79432487)
p_AL <- 2324/N_A
I_AL <- 2*0.42211704*(1-0.42211704)
p_AR <- 8037/N_A
I_AR <- 2*0.90195347*(1-0.90195347)
N_A*(I_A - p_AL*I_AL - p_AR*I_AR)

# improve at node 2:
N_A <- 2324
I_A <- 2*0.42211704*(1-0.42211704)
p_AL <- 1673/N_A
I_AL <- 2*0.32994620*(1-0.32994620)
p_AR <- 651/N_A
I_AR <- 2*0.65898618*(1-0.65898618)
N_A*(I_A - p_AL*I_AL - p_AR*I_AR)
```
We will see that this matches what we have below.  Then, the unscaled variable importance score for variable $k$ is the sum of these improve scores for any split in which the variable is used.  That is, if the optimal split at node A is $C(A)$, then
$$
VI_{k, unsc} = \sum_{A}I[C(A) = C_k(A)] \Delta_I( C_{k}(A), A).
$$
The scaled variable importance scores rescale everything so that they sum to 100:
$$
VI_{k, sc} = 100 \times \frac{VI_{k, unsc}}{\sum_k VI_{k, unsc}}.
$$
Okay, now we can look at the output for the control potential outcomes tree:
```{r}
summary(tree_y0$tree)
```

## Potential outcome heterogeneity and effect heterogeneity

What are the implications of a covariate's importance for predicting potential outcomes when it comes to predicting effect heterogeneity?  We have
$$
E[Y(1)-Y(0)|X] = E[Y(1)|X] - E[Y(0)|X]
$$
by the linearity of expectations.  That being the case, if $E[Y(1)|X] = E[Y(1)]$ and $E[Y(0)|X] = E[Y(0)]$, then it must be that $E[Y(1)-Y(0)|X]= E[Y(1)-Y(0)]$. As such, being a variable that is predictive of either of the potential outcomes is a *necessary* condition for a covariate for it to be predictive of effects.  That said, variables that are predictive of potential outcomes may not be predictive of effects.  E.g., we could have that 
$$
E[Y(d)|X] = \alpha + \beta d + \gamma X 
$$
for $d=0,1$, in which case 
$$
E[Y(1)-Y(0)|X] = (\alpha + \beta + \gamma X) - (\alpha + \gamma X) = \beta,
$$
which does not depend on $X$. That said, a starting point for selecting variables to consider as contributors to effect heterogeneity would those that have some predictive relationship with potential outcomes. (Note that this is similar to what is done in Imai and Strauss 2011).

Let us view the predicted potential outcome trees, plotting over the two covariates that seem the matter:
```{r, echo=FALSE}
mex_data$y1_tree <- predict(tree_y1$tree,
                                  newdata=mex_data,
                                  type="prob")[,"1"]
mex_data$y0_tree <- predict(tree_y0$tree,
                                  newdata=mex_data,
                                  type="prob")[,"1"]
mex_data$tau_tree <- mex_data$y1_tree - mex_data$y0_tree

par(mfrow=c(3,4), mar=c(4,4,2,2))
for(ageUp in 6:16){
mexdataUp <- subset(mex_data, ml_age==ageUp)
orderBy <- order(mexdataUp$ml_yrs_educ)
plot(jitter(mexdataUp$ml_yrs_educ[orderBy]),
     jitter(mexdataUp$y1_tree[orderBy]),
     col="red",
     xlab="Yrs educ",
     ylab="Pr(enroll)",
     ylim=c(0,1),
     xlim=c(0,12),
     main=paste0("Age",ageUp),
     cex=.5)
points(jitter(mexdataUp$ml_yrs_educ[orderBy]),
     jitter(mexdataUp$y0_tree[orderBy]),
     col="blue",
     cex=.5)
}
```
From these plots we can see that for lower age groups, the predicted means are very close, and so effects are basically zero.  Then, we have variation for higher age groups.  We can see all of this more clearly by plotting the estimated conditional treatment effects themselves:

```{r, echo=FALSE}
par(mfrow=c(3,4), mar=c(4,4,2,2))
for(ageUp in 6:16){
mexdataUp <- subset(mex_data, ml_age==ageUp)
orderBy <- order(mexdataUp$ml_yrs_educ)
plot(jitter(mexdataUp$ml_yrs_educ[orderBy]),
     jitter(mexdataUp$tau_tree[orderBy]),
     xlab="Yrs educ",
     ylab="ATE hat",
     ylim=c(-1,1),
     xlim=c(0,12),
     main=paste0("Age",ageUp),
     cex=.5)
}

```

While this approach is useful for getting a handle on effect heterogeneity, it is not optimized with respect to treatment effects per se.  We will turn to such methods next.

\clearpage
## Trees for effect heterogeneity

We now turn to trees for effect heterogeneity. There are different ways to go about this.  The first is to use Horvitz-Thompson scaling. The next to construct a tree optimized with respect to effect heterogeneity.

### Horvitz-Thompson scaling

Recall the HT scaling result above. We can construct our unbiased signal of $s_0(X)$ as
$$
\hat{S}(X_i) = Y_i\frac{(D_i-p(X_i))}{p(X_i)(1-p(X_i))}. 
$$
We assume complete random assignment, in which case we have, $p(X_i) = p$. For the Progresa experiment:
```{r}
pUp <- mean(mex_data$treatment)
pUp
mex_data$Senrolled <- with(mex_data, enrolled*((treatment-pUp)/(pUp*(1-pUp))))
```

Then we fit the tree to this:
```{r}
mexFormula_S <- as.formula(paste("Senrolled ~", paste(covs, collapse="+")))
tree_S <- rpartOp(formulaIn=mexFormula_S,
                    methodIn="class",
                    dataIn=mex_data,
                    xvalIn=10,
                    minbucketIn=2,
                    cp_incIN=0,
                    target="po")
rpart.plot(tree_S$tree)
par(mar=c(4,10,2,2))
rpart_impplot(treeIn=tree_S, mainIn="S-HT", labelScale=0.6)
```

Pretty much the same story as what we had seen before.  

### Causal Tree

We now turn to the "causal tree," as per Athey and Imbens.  We use their recommended causal tree (CT) splitting rule.  We do not use their honest splitting or CV rules because we are not interested in in-sample inference.
```{r, eval=TRUE}
tree_fx_tree <- causalTree(mexFormula, 
                      data=mex_data,
                      treatment=mex_data$treatment,
                      split.Rule="CT",
                      split.Honest=F,
                      cv.option="CT",
                      cv.Honest=F,
                      split.alpha=1,
                      control=rpart.control(xval=10, 
                                         minbucket=2,
                                         cp=0.00005)) 
opcp_fx <- tree_fx_tree$cptable[,1][which.min(tree_fx_tree$cptable[,4])]
optreeOut_fx <- prune(tree_fx_tree, opcp_fx)
rpart.plot(optreeOut_fx)

tree_fx <- list(tree = optreeOut_fx,
                varimp = 100*(optreeOut_fx$variable.importance/sum(optreeOut_fx$variable.importance)))
par(mar=c(4,10,2,2))
rpart_impplot(treeIn=tree_fx, mainIn="S-Causal Tree", labelScale=0.6)
```

Similar story as above.

Note that the CV-optimal tree and the variable importance ranking don't quite match in that the latter puts some weight on literacy.  This is because the variable importance measure looks at all trees where it is included in a split.  But presumably what it offers is redundant relative to something else that is included in the optimal tree.
