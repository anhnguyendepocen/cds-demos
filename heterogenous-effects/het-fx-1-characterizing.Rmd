---
title: "Characterizing Effect Heterogeneity 1"
author: "Cyrus Samii"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
    includes:
      in_header: preamble.tex
indent: true
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Paths to wd for users
knitr::opts_knit$set(root.dir="~/documents/github/cds-demos/heterogenous-effects")
library(rio)
library(knitr)
library(Emcdf)
source("~/documents/github/cds-demos/analysis-functions/analysis-functions.R")
```

\clearpage

# Introduction

Below are notes on approaches to characterizing effect heterogeneity. The methods are reviewed in light of the goals in the paper, Gechter et al. (2019, [arxiv](https://arxiv.org/abs/1806.07016)).  It would be worth becoming familiar with that paper before reviewing what I have below.

# Measuring the Extent of Effect Heterogeneity

We will start with Heckman, Smith, and Clements (1997) examination of the binary outcomes case. Let's bring in the data from the Progresa to illustrate. We have the marginal enrollment distributions under treatment and control,

```{r}
progd <- as.data.frame(import("for_analysis_el.dta"))
progd$treat <- 1-progd$control
enTab <- t(crossTab(progd$enrolled, progd$treat, "Enrolled", "Treated")[[2]])
colnames(enTab) <- rep("", ncol(enTab))
kable(enTab, align="r")
```

and the corresponding treatment effect:
```{r}
ATE <- mean(progd$enrolled[progd$treat==1]) - mean(progd$enrolled[progd$treat==0])
ATE
```

Our aim is to fill in a contingency table for principal strata, which are defined in terms of whether units are enrolled ($E$) or not enrolled ($N$) under control and then under treatment.  The share of units enrolled under both is $P_{EE}$ and so on. The full contingency table looks like the following:

\begin{center}
\begin{tabular}{cc|cc|c}
\multicolumn{5}{c}{Control}\\
& & $E$ & $N$ & \\
\hline
Treated & $E$ & $P_{EE}$ &$P_{EN}$ & $P_{E\cdot}$\\ 
 & $N$ & $P_{NE}$ & $P_{NN}$ & $P_{N\cdot}$\\ 
\hline
 &  & $P_{\cdot E}$ & $P_{\cdot N}$ & $P_{N\cdot}$\\ 
\end{tabular}
\end{center}

The ATE is equivalent to,
\begin{align*}
ATE & = P_{E \cdot} - P_{\cdot E} \\
& = (P_{EE} + P_{EN}) -  (P_{EE} +  P_{NE})\\
& = P_{EN} - P_{NE}
\end{align*}

The question is whether the experimental data allow us to identify these principal strata shares.  Generally speaking, with no assumptions, the answer is no.  

So let's start to consider some assumptions.  Suppose, for example, that the effects of the intervention are weakly monotonic in that they either increase or do not change enrollment. Then,  $P_{NE} = 0$, in which case $ATE = P_{EN}$.  As such, for the share given by $ATE$, the intervention has an effect of 1, and for the share given by $1-ATE$, the intervention has no effect.  In our application, under such weakly monotonic beneficial effects, `r 100*round(ATE, 3)`\% of the population would have effects of 1, and `r 100*round(1-ATE, 3)`\% would have effects of 0.

Such monotonicity is a strong assumption, however, and so we now turn to more general bounds.

## Frechet-Hoeffding bounds

A general bound can be derived a la Frechet-Hoeffding. We  explain this by way of a lesson on copulas, upon which Frechet-Hoeffding bounds are based.  

### A digression on copulas

Suppose two potential outcomes $Y(0),Y(1)$ with a joint distribution $G(y_0,y_1)$ and marginal distributions $F_0(y)$ and $F_1(y)$. As such, $F_0(0) =  P_{\cdot N}$ and $F_0(1) = 1$, while $F_1(0) = P_{N \cdot}$ and $F_1(1)=1$.  We can draw these for our data:
\begin{center}
```{r, out.width = "200px", echo=FALSE}
plot(ecdf(progd$enrolled[progd$treat==0]), main="Control", ylab="F0")
plot(ecdf(progd$enrolled[progd$treat==1]), main="Treated", ylab="F1")
```
\end{center}

Note that given the discrete random variables, we use the generalized inverse of the CDF:
$$
F^{-1}(p) = \inf\{x \in \mathbb{R}:F(x) \ge p \},
$$
that is, it returns the smallest $x$ that returns $F(x) \ge p$. So,  $F^{-1}_0(1)=1$, we choose $y_0=1$. Similarly, $F^{-1}_0(u)=1$ for $P_{\cdot N} < u < 1$.  More generally for generalized inverse function,
\begin{itemize}
\item $F(F^{-1}(u)) \ge u$.
\item $F(x) \ge u$ iff $x \ge F^{-1}(u)$.
\item For $U\sim U[0,1]$, $X = F^{-1}(U)$ has CDF F.
\end{itemize}

Note how a CDF transforms a random variable. E.g, 
```{r, out.width = "400px"}
X <- rnorm(1000)
Y <- pnorm(X)
par(mfrow=c(1,3), pty="s")
hist(X)
hist(Y)
plot(X, Y)
```

Now let us define a copula and study its properties.  For our case, we define it as mapping from two marginal distributions to range of a joint distribution.  Generically, a copula for a bivariate distribution can be defined as $C:[0,1]^2 \rightarrow [0,1]$ such that for $(U_1, U_2)$ with margins that are distributed $Unif[0,1]$, we have,
$$
C(u_1, u_2) = \Pr[U_1 \le u_1, U_2 \le u_2].
$$

Given this formulation, a few things follow:
\begin{itemize}
\item If $u_1=0$ or $u_2=0$ then $C(u_1,u_2) = 0$. The reason is that fixing one of the arguments to zero sets us at the edge of the cdf $C(\cdot)$ and so the mass is zero along this edge.
\item $C(1, u_2) = u_2$ and $C(u_1, 1) = u_1$.  The reason is that being at the 1-edge for $u_j$ of the cdf $C(\cdot)$ implies that we have already incorporated all of the mass in the $j$ dimension, meaning that we are just varying mass in the $i$ dimension.
\item If $a_j \le b_j$, then
$$
(C(b_1, b_2) - c(a_1, b_2)) - (C(b_1, a_2) - C(a_1, a_2)) \ge 0.
$$
This is a sort of monotonicity.
\item $C$ is non-decreasing in its arguments.
\item $C$ is cts (because it is Lipschitz).
\end{itemize}

Sklar's theorem I says that we can construct multivariate CDFs using copulas. Here we state it for the bivariate case.
\begin{quote}
Let $C$ be a bivariate copula, and suppose univariate CDFs $F_0$ and $F_1$.  Then,
$$
F(y_0, y_1) = C(F_0(y_0),F_1(y_1)) 
$$
is a bivariate CDF with margins $F_0$ and $F_1$. 
\end{quote}
Thus, the function $C$ gives rise to a bivariate CDF with marginals $F_0$ and $F_1$. The proof is very simple. Suppose $(U_0, U_1) \sim C$, the function defined in the proof.  What are the marginals of this distribution when the arguments are defined as in the proof?  Well, if $U_0 = F_0(Y_0)$, say, then $Y_0 = F_0^{-1}(U_0) \sim F_0$; similarly $Y_1 = F_1^{-1}(U_1) \sim F_1$.

Sklar's theorem II says that any multivariate CDF has a copula. Again we state for the bivariate case:
\begin{quote}
If $F$ is a bivariate CDF with marginals $F_0$ and $F_1$, then there exists a copula $C$ such that Sklar I holds.  Moreover, if the margins are continuous, then $C$ is unique and equals,
$$
C(u_0, u_1) = F(F_0^{-1}(u_0), F_1^{-1}(u_1)).
$$
\end{quote}
The proof is as follows. Suppose the margins are continuous. Let $(Y_0, Y_1) \sim F$. Now, $U_j = F_j(Y_j) \sim Unif[0,1]$.  Then, $(U_0, U_1)\sim C$ as defined in Sklar I holds. 

All of these properties hold for discrete variables as well, although an issue with discrete variables is that the copula for a given joint distribution may not be unique.

See for example a bivariate normal case.  

#### Bivariate normal distribution

```{r, echo=FALSE, out.width = "500px"}
library(mvtnorm)
Sig <- matrix(c(1,.75,.75,1), ncol=2)
Y <- rmvnorm(n=1000,
             sigma=Sig)

Y0 <- Y[,1]
Y1 <- Y[,2]
F0 <- pnorm(Y0)
F1 <- pnorm(Y1)

Finv0 <- qnorm(F0)
Finv1 <- qnorm(F1)

par(mfrow=c(2,2), pty="s", mar=c(4,2,2,2))
hY0 <- hist(Y0, plot=FALSE)
barplot(hY0$density)
plot(c(0,1),c(0,1),
     type="n", 
     axes=F, 
     xlab="",
     ylab="")
text(.5,.5, "Bivariate Normal.\n Shading is joint \n CDF values.")
Fy0y1 <- apply(cbind(Y0,
                     Y1),
          1,
        function(x){
          pmvnorm(upper=x,
                   sigma=Sig)[1] 
          })

plot(Y0, Y1,
     col=gray(.9*(1-Fy0y1)))
hY1 <- hist(Y1, plot=FALSE)
barplot(hY1$density, horiz=T)
```

#### Corresponding copula
```{r, echo=FALSE, out.width = "500px"}
par(mfrow=c(2,2), pty="s", mar=c(4,2,2,2))
hF0 <- hist(F0, plot=FALSE)
barplot(hF0$density)
plot(c(0,1),c(0,1),
     type="n", 
     axes=F, 
     xlab="",
     ylab="")
text(.5,.5, "Joint distn of U. \n Shading is copula \n values.")

Umat <- cbind(F0,F1)
CF <- apply(Umat,
      1,
      function(x){
        emcdf(Umat, x)
      })
plot(F0, F1,
     col=gray(.9*(1-CF)))
hF1 <- hist(F1, plot=FALSE)
barplot(hF1$density, horiz=T)
```

#### Illustrating of Sklar II, from CDF to copula
\begin{center}
```{r, echo=FALSE, out.width = "500px"}
par(mfrow=c(2,2), pty="s", mar=c(4,2,2,2))
plot(Finv0, F0)
 plot(c(0,1),c(0,1),
      type="n", 
      axes=F, 
      xlab="",
      ylab="")
text(.5,.5, "Start with bivariate \n uniform RV U. Apply \n  inverse marginal\n CDFs F0,F1. \n Apply joint CDF\n F.  Values (gray) \n equal C(U).",
     cex=1)
FFinv <- apply(cbind(Finv0,
                     Finv1),
          1,
        function(x){
          pmvnorm(upper=x,
                   sigma=Sig)[1] 
          })
 plot(Finv0, Finv1,
     col=gray(.9*(1-FFinv)))
plot(F1, Finv1)
```
\end{center}

We can thus see that the $C$ and $F$ values are the same:
\begin{center}
```{r, out.width = "200px"}
plot(FFinv, Fy0y1)
```
\end{center}

#### Bounds

Okay, now we have a sense of copulas and how they relate to CDFs.  Here is the statement for the Frechet-Hoeffding bounds:
\begin{quote}
Any bivariate copula $C(u,v)$ verifies,
$$
\max(u + v - 1, 0 ) \le C(u,v) \le \min(u,v).
$$
\end{quote}
The proof is as follows.  By the properties of copulas, we have
$$
C(u,v) \le C(u,1) \le u 
$$
and
$$
C(u,v) \le C(1,v) \le v, 
$$
which establishes the upper bound, $\min(u,v)$.  Then, by the ``monotonicity'' property described above, defining $a_1=u$,$b_1 = 1$, $a_2=v$, and $b_2=1$, we have
$$
(C(1, 1) - C(u, 1)) - (C(1, v) - C(u, v)) = 1 - u - v +C(u,v) \ge 0,
$$
which yields the lower bound.

For intuition, recall 
$$
C(u,v) = \Pr[U\le u, V \le v]
$$
for $U,V$ with uniform marginals on $[0,1]$.  Then, the Frechet-Hoeffding bound is based on situations where $U$ and $V$ are perfectly correlated and perfectly anti-correlated.  Suppose they are perfectly correlated.  Then, $V=U$ and $C(u,v) = \Pr[U \le u, U \le v] = \min(u,v)$.  Now suppose they are perfectly anti-correlated.  Then, $V = 1-U$ and $C(u,v) = \Pr[U \le u, U \ge 1-v]$. If $u < 1-v \Rightarrow u+v-1 >0$, then this equals 0. Otherwise, it equals the space between $u$ and $1-v$, which is $u - (1-v) = u+v-1$.  

Now, the idea is to go from this result on copulas to a result on joint distributions.  This is where Sklar I comes into play.  Recall that it states that
$$
F(y_0, y_1) = C(F_0(y_0), F_1(y_1))
$$
is a bivariate CDF with marginals $F_0$ and $F_1$.  As such, we substitute the arguments $F_0(y_0)$ and $F_1(y_1)$ for $u$ and $v$ and then $F(y_0, y_1)$ for $C(u,v)$ n the statement of the bounds to obtain,
$$
\max(F_0(y_0) + F_1(y_1) - 1, 0 ) \le F(y_0, y_1) \le \min(F_0(y_0), F_1(y_1)).
$$

Relating the correlation and anti-correlation from the copulas to this result, we have that the upper bound is reached when $Y_0$ and $Y_1$ are comonotonic (i.e., $Y_1$ is a deterministic non-decreasing function of $Y_0$, implying a rank correlation of 1) and the lower bound then they are countermonotonic (i.e., $Y_1$ is a deterministic non-increasing function of $Y_0$, implying a rank correlation of -1).

### Applying Frechet-Hoeffding Bounds

Returning to our example, recall that we are interested in the joint distribution of outcomes under treatment and control, where the outcome are each binary (either enrolled or not enrolled).  Recall as well that we defined, e.g., $P_{EE}$ to refer to the share of individuals for which potential outcomes under both control and treatment are ``enrolled.''  Let us define two indicator variables, $E_0$ and $E_1$, for whether an individual is enrolled in treatment and control, respectively.  Then $P_{EE}$ is the joint distribution for $E_1$ and $E_0$, while $P_{\cdot E}$ is the marginal distribution of $E_0$ and $P_{E \cdot}$ is the marginal distribution of $E_1$.  We can proceed similarly for $P_{EN}$, $P_{NE}$, and $P_{NN}$. Then, applying the Frechet-Hoeffding bounds we have, following Heckman et al. (1997), we have
\begin{align*}
\max[P_{E\cdot} + P_{\cdot E} - 1, 0] & \le P_{EE} \le \min[P_{E \cdot}, P_{\cdot E}]\\
\max[P_{E\cdot} - P_{\cdot E}, 0] & \le P_{EN} \le \min[P_{E \cdot}, 1- P_{\cdot E}]\\
\max[-P_{E\cdot} + P_{\cdot E}, 0] & \le P_{NE} \le \min[1-P_{E \cdot},  P_{\cdot E}]\\
\max[1-P_{E\cdot} - P_{\cdot E}, 0] & \le P_{NN} \le \min[1-P_{E \cdot},  1-P_{\cdot E}].\\
\end{align*}
From the table above, we see that in our application we have the following: 
```{r, echo=F}
PEd <- mean(progd$enrolled[progd$treat==1])
PdE <- mean(progd$enrolled[progd$treat==0])

boundsOut <- rbind(c(max(PEd + PdE-1, 0), min(PEd, PdE)),
                  c(max(PEd - PdE, 0), min(PEd, 1-PdE)),
                  c(max(-PEd + PdE, 0), min(1-PEd, PdE)),
                  c(max(1-PEd - PdE, 0), min(1-PEd, 1-PdE)))
colnames(boundsOut) <- c("Lower", "Upper")
rownames(boundsOut) <- c("EE","EN","NE","NN")
kable(boundsOut)
```

So as many as 20.5\% and as few as 3.8\% may have been induced to enroll while as many as 16.8\% and as few as 0\% may have been induced not to enroll.  Now, we have the identity $ATE = P_{EN} - P_{NE}$, so high $P_{EN}$ require high $P_{NE}$ and vice versa.  Given $ATE = 0.038$, we can construct the range of joint  $(P_{EN}, P_{NE})$ values consistent with our ATE estimate:
\begin{center}
```{r, echo=FALSE, out.width = "200px"}
EN_seq <- seq(max(PEd - PdE, 0), min(PEd, 1-PdE), 
              length=20) 
NE_seq <- EN_seq - ATE
par(pty="s")
plot(EN_seq,NE_seq,
     xlab="P_EN (helped)", ylab="P_NE (hurt)", 
     type="l",
     xlim=range(c(EN_seq, NE_seq)),
     ylim=range(c(EN_seq, NE_seq)))
```
\end{center}

Based on these results, the maximal effect heterogeneity would be where `r round(EN_seq[length(EN_seq)], 3)*100`\% of the population would be helped, `r round(NE_seq[length(NE_seq)], 3)*100`\% would be hurt, and then the remainder (`r round(1-EN_seq[length(EN_seq)]-NE_seq[length(NE_seq)], 3)*100`\%) would not be affected.

We can also compute these upper bounds on the extent of effect heterogeneity by strata.  The terms that we need to compute are the treatment and control enrollment indicator means ($P_{E\cdot}$ and $P_{\cdot E}$) in each stratum.

```{r, echo=FALSE}
fit_agesex <- lm(enrolled~treat*as.factor(age)*male,
                  data=progd)
ageVec <- 6:16
predMat1 <- data.frame(age=c(ageVec, ageVec),
                       male=c(rep(0, length(ageVec)), 
                              rep(1, length(ageVec))),
                    treat=rep(1, 2*length(ageVec)))
predMat0 <- predMat1
predMat0$treat <- 0
agesexMeans <-cbind(predMat1[,c("age","male")],
              predict(fit_agesex, newdata=predMat0),
              predict(fit_agesex, newdata=predMat1),
              predict(fit_agesex, newdata=predMat1)-predict(fit_agesex, newdata=predMat0))
colnames(agesexMeans) <- c("Age","Male","Control","Treated","CATE")
agesexMeans.d <- as.data.frame(agesexMeans)
agesexMeans.d$PEN_ub <- apply(agesexMeans.d[,c("Control","Treated")],
                                1,
                                function(x){
                                    min(x[2],1-x[1])
                                })
agesexMeans.d$PNE_ub <- apply(agesexMeans.d[,c("Control","Treated")],
                                1,
                                function(x){
                                    min(1-x[2],x[1])
                                })
kable(round(agesexMeans.d, 2))
```


The table above shows outcome means, the conditional average treatment effect (CATE), and then the upper bounds on the share of those induced to enroll ($EN$) and those induced not to enroll $NE$) for strata that distinguish between girls and boys aged between 6 and 16.  The $EN$ and $NE$ shares are the upper bounds of the shares of units within the given stratum for which there may be a treatment effect.  If effect monotonicity holds such that the treatment never induces anyone not to enroll, then the share of people within the given stratum that might be induced to enroll is equal to the value of the CATE.  

When thinking about optimal treatment regimes, if the sum of $EN$ and $NE$ is small, then the potential for different treatment regimes to produce different outcome distributions is quite limited.  In our application, for younger children, such limits are quite apparent.  It is only for older youth, above the age of 12, where there is even the potential for there to be effect heterogeneity that is at all substantial.  Under effect monotonicity the limits are even tighter.

## Decomposing systematic and idiosyncratic treatment variation

Ding, Feller, and Miratrix (2019, *JASA*) consider the following decomposition of treatment effects:
$$
\tau_i = Y_i(1)-Y_i(0) = X_i\beta + \epsilon_i,
$$
where $\beta$ is defined as the finite population OLS coefficient of the regression of $\tau_i$ on $X_i$.  Then $X_i\beta$ is the systematic treatment effect variation explained by $X$ and $\epsilon_i$ is the idiosyncratic variation.  Note that if we could observe the $\tau_i$, then we would have that
$$
\beta = \gamma_1 - \gamma_0,
$$
where $\gamma_1$ and $\gamma_0$ are the finite population coefficients from the regressions of the potential outcomes on the covariates, in which case $\epsilon_i$ is the differences in residuals from those two regressions.  We cannot compute these quantities directly, but we can estimate them without bias (assuming random assignment) by working with regression fits to the treatment and control observations separately.  These results motivate a test whereby on fits an interacted regression and then does a joint test on the coefficients for the terms that interact the treatment with covariates.  This is all very natural -- what is distinct in Ding et al.'s contribution is that their inferential methods are based solely on the randomization distribution.  They also bound the extent of idiosyncratic variation using FH bounds like above. This is like what Djebbari and Smith (2008) do, essentially apply FH bounds to the residuals from an interacted regression.  

I won't go into more detail on this now.

